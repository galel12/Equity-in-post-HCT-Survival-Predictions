{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf4a99a",
   "metadata": {},
   "source": [
    "<center><img src=\"assets/header.png\" width=\"25%\" style=\"border-radius:20px;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80a941",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10141ab",
   "metadata": {},
   "source": [
    "We are Gal Elharar (207906553) and Roy Wolfer (324074566), computer science students with a strong passion for data science and its applications in healthcare.  \n",
    "For our semester project, we sought to tackle a meaningful challenge while expanding our skill set.  \n",
    "After exploring Kaggle competitions, we were immediately drawn to the [Equity in post-HCT Survival Predictions](https://www.kaggle.com/competitions/equity-post-HCT-survival-predictions) challenge.\n",
    "\n",
    "This competition focuses on predicting survival probabilities for patients undergoing hematopoietic cell transplantation (HCT),  \n",
    "with an emphasis on ensuring equitable outcomes across different racial groups.  \n",
    "The evaluation metric used in this challenge is the Stratified Concordance Index (C-index),  \n",
    "which adjusts for racial stratification to promote fairness and equity in the predictions.  \n",
    "This makes the challenge particularly impactful in addressing disparities in healthcare outcomes.\n",
    "\n",
    "We are excited to dive into this project, combining our data science skills with our interest in advancing equity in medicine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1213278",
   "metadata": {},
   "source": [
    "### The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de3df4",
   "metadata": {},
   "source": [
    "We couldn’t explain the problem any better as it was outlined in the kaggle challenge description:\n",
    "\n",
    "Improving survival predictions for allogeneic HCT patients is a vital healthcare challenge.  \n",
    "Current predictive models often fall short in addressing disparities related to socioeconomic status, race, and geography.  \n",
    "Addressing these gaps is crucial for enhancing patient care, optimizing resource utilization, and rebuilding trust in the healthcare system.\n",
    "\n",
    "This competition aims to encourage participants to advance predictive modeling by ensuring that survival predictions are both precise and fair for patients across diverse groups.  \n",
    "By using synthetic data, which mirrors real-world situations while protecting patient privacy, participants can build and improve models that more effectively consider diverse backgrounds and conditions.\n",
    "\n",
    "You’re challenged to develop advanced predictive models for allogeneic HCT that enhance both accuracy and fairness in survival predictions.  \n",
    "The goal is to address disparities by bridging diverse data sources, refining algorithms, and reducing biases to ensure equitable outcomes for patients across diverse race groups.  \n",
    "Your work will help create a more just and effective healthcare environment, ensuring every patient receives the care they deserve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc700fed",
   "metadata": {},
   "source": [
    "### Risk Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4848f",
   "metadata": {},
   "source": [
    "In the “Equity in post-HCT Survival Predictions” challenge, survival predictions are quantified as risk scores.  \n",
    "These scores represent the likelihood of an adverse event (i.e., death, relapse, rejection, or moderate to severe GVHD) occurring within a specific timeframe after hematopoietic cell transplantation (HCT).  \n",
    "To evaluate the predictions, the risk scores are compared to actual survival outcomes, and their discrimination ability is assessed using the Stratified Concordance Index (C-index)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70a379",
   "metadata": {},
   "source": [
    "##### Key Details:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f7192",
   "metadata": {},
   "source": [
    "1. Risk Score Scale:\n",
    "    - A higher risk score indicates a higher likelihood of the target event (e.g., shorter survival time or higher mortality risk).\n",
    "    - A lower risk score suggests a lower probability of the target event (e.g., longer survival time or lower mortality risk).\n",
    "2. Stratification for Fairness:\n",
    "    - The risk scores are stratified across different racial groups to evaluate fairness.\n",
    "    - Consistent and unbiased predictions across groups are critical for equitable healthcare applications.\n",
    "3. Clinical Implications:\n",
    "    - These risk scores may guide clinical decision-making by identifying patients at higher risk, enabling personalized follow-up and care."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf276840",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c8930",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-22T18:02:36.281278Z",
     "iopub.status.busy": "2024-12-22T18:02:36.280828Z",
     "iopub.status.idle": "2024-12-22T18:02:39.665804Z",
     "shell.execute_reply": "2024-12-22T18:02:39.664605Z"
    },
    "papermill": {
     "duration": 3.39692,
     "end_time": "2024-12-22T18:02:39.668494",
     "exception": false,
     "start_time": "2024-12-22T18:02:36.271574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import xgboost\n",
    "import catboost\n",
    "import warnings\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from scipy.stats import rankdata\n",
    "from itables import show\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, quantile_transform\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "import statsmodels\n",
    "\n",
    "all_model_scores = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55846537",
   "metadata": {
    "papermill": {
     "duration": 0.007127,
     "end_time": "2024-12-22T18:02:39.683046",
     "exception": false,
     "start_time": "2024-12-22T18:02:39.675919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Reading the data\n",
    "\n",
    "We read the data and observe:\n",
    "1. The training dataset has 59 columns, many of which are categorical and have missing values.\n",
    "2. Two columns are missing from the test dataset: `efs` and `efs_time`. These two columns together make up the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb840402",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-22T18:02:39.700360Z",
     "iopub.status.busy": "2024-12-22T18:02:39.699444Z",
     "iopub.status.idle": "2024-12-22T18:02:40.176138Z",
     "shell.execute_reply": "2024-12-22T18:02:40.174593Z"
    },
    "papermill": {
     "duration": 0.488627,
     "end_time": "2024-12-22T18:02:40.178821",
     "exception": false,
     "start_time": "2024-12-22T18:02:39.690194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('datasets/train.csv', index_col='ID')\n",
    "test = pd.read_csv('datasets/test.csv', index_col='ID')\n",
    "data_dictionary = pd.read_csv('datasets/data_dictionary.csv')\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ca403c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T18:02:40.196833Z",
     "iopub.status.busy": "2024-12-22T18:02:40.196385Z",
     "iopub.status.idle": "2024-12-22T18:02:40.358811Z",
     "shell.execute_reply": "2024-12-22T18:02:40.357795Z"
    },
    "papermill": {
     "duration": 0.174764,
     "end_time": "2024-12-22T18:02:40.361457",
     "exception": false,
     "start_time": "2024-12-22T18:02:40.186693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [f for f in test.columns if f != 'ID']\n",
    "\n",
    "race_groups = np.unique(train.race_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2261bf8",
   "metadata": {
    "papermill": {
     "duration": 0.007307,
     "end_time": "2024-12-22T18:02:40.376501",
     "exception": false,
     "start_time": "2024-12-22T18:02:40.369194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The data dictionary\n",
    "\n",
    "The data dictionary simply describes the 59 columns of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "260ea507",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-22T18:02:40.393763Z",
     "iopub.status.busy": "2024-12-22T18:02:40.392662Z",
     "iopub.status.idle": "2024-12-22T18:02:40.410371Z",
     "shell.execute_reply": "2024-12-22T18:02:40.409319Z"
    },
    "papermill": {
     "duration": 0.029965,
     "end_time": "2024-12-22T18:02:40.413865",
     "exception": false,
     "start_time": "2024-12-22T18:02:40.383900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(data_dictionary, scrollY=\"400px\", scrollCollapse=True, paging=False, classes=\"display compact\", columnDefs=[{\"className\": \"dt-left\", \"targets\": \"_all\"}], layout={\"topStart\": \"search\", \"topEnd\": None})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955cede",
   "metadata": {
    "papermill": {
     "duration": 0.008525,
     "end_time": "2024-12-22T18:02:40.431514",
     "exception": false,
     "start_time": "2024-12-22T18:02:40.422989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Race group distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe58b2",
   "metadata": {
    "_kg_hide-input": false,
    "papermill": {
     "duration": 0.008475,
     "end_time": "2024-12-22T18:02:40.448940",
     "exception": false,
     "start_time": "2024-12-22T18:02:40.440465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the training data, there are six race groups with about 4800 samples each.  \n",
    "Because in no country of the world these six race groups occur with equal frequencies, we know that some of the groups have been upsampled or downsampled in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea602f",
   "metadata": {},
   "source": [
    "Note: The balanced representation of race groups in this dataset suggests that it has been engineered to ensure equal representation rather than reflecting real-world distributions. While this helps create fairer models, it also means that results might not generalize well to actual clinical populations. Any findings should be interpreted with caution, especially when assessing disparities across racial groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ca7e46",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-22T18:02:40.468166Z",
     "iopub.status.busy": "2024-12-22T18:02:40.467713Z",
     "iopub.status.idle": "2024-12-22T18:02:40.684728Z",
     "shell.execute_reply": "2024-12-22T18:02:40.683257Z"
    },
    "papermill": {
     "duration": 0.234291,
     "end_time": "2024-12-22T18:02:40.691841",
     "exception": false,
     "start_time": "2024-12-22T18:02:40.457550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "race_counts = train.race_group.value_counts()\n",
    "\n",
    "plt.pie(race_counts.values, labels=race_counts.index, autopct='%1.1f%%', \n",
    "        startangle=140, colors=plt.cm.Paired.colors, wedgeprops={'edgecolor': 'black'})\n",
    "\n",
    "plt.title('Race Group Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e34b35",
   "metadata": {},
   "source": [
    "### Statistical Tests\n",
    "\n",
    "Let's perform various statistical tests to validate our observations and understand relationships in the data:  \n",
    "1. Chi-square tests for categorical variables to check if distributions differ across race groups  \n",
    "2. ANOVA tests for numerical variables to check for significant differences across race groups  \n",
    "3. Correlation analysis between numerical features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978595b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square tests for categorical variables vs race_group\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "cat_vars = ['conditioning_intensity', 'graft_type', 'cmv_status', 'sex_match']\n",
    "chi_results = {}\n",
    "\n",
    "print(\"Chi-square tests for categorical variables vs race_group:\")\n",
    "print(\"-\" * 50)\n",
    "for var in cat_vars:\n",
    "    # Create contingency table\n",
    "    contingency = pd.crosstab(train['race_group'], train[var])\n",
    "    # Perform chi-square test\n",
    "    chi2, p_val, dof, expected = chi2_contingency(contingency)\n",
    "    chi_results[var] = {'chi2': chi2, 'p_value': p_val}\n",
    "    print(\"{var}:\")\n",
    "    print(\"Chi-square statistic: {chi2:.2f}\")\n",
    "    print(\"p-value: {p_val:.2e}\")\n",
    "    print(\"Significant: {'Yes' if p_val < 0.05 else 'No'}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c0da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA tests for numerical variables across race groups\n",
    "from sklearn.feature_selection import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "num_vars = ['age_at_hct', 'donor_age', 'comorbidity_score', 'karnofsky_score']\n",
    "\n",
    "print(\"ANOVA tests for numerical variables across race groups:\")\n",
    "print(\"-\" * 50)\n",
    "for var in num_vars:\n",
    "    # Collect groups\n",
    "    groups = [group[var].dropna() for name, group in train.groupby('race_group')]\n",
    "    # Perform ANOVA\n",
    "    f_stat, p_val = f_oneway(*groups)\n",
    "    print(\"{var}:\")\n",
    "    print(\"F-statistic: {f_stat:.2f}\")\n",
    "    print(\"p-value: {p_val:.2e}\")\n",
    "    print(\"Significant: {'Yes' if p_val < 0.05 else 'No'}\")\n",
    "    # If significant, perform Tukey's HSD test\n",
    "    if p_val < 0.05:\n",
    "        print(\"Tukey's HSD test results:\")\n",
    "        tukey = pairwise_tukeyhsd(endog=train[var].dropna(),\n",
    "                                 groups=train['race_group'].dropna(),\n",
    "                                 alpha=0.05)\n",
    "        print(tukey)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis between numerical variables\"\n",
    "from sklearn.isotonic import spearmanr\n",
    "\n",
    "numerical_vars = ['age_at_hct', 'donor_age', 'comorbidity_score', 'karnofsky_score']\n",
    "\n",
    "print(\"Spearman correlation tests between numerical variables:\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(numerical_vars)):\n",
    "    for j in range(i+1, len(numerical_vars)):\n",
    "        var1, var2 = numerical_vars[i], numerical_vars[j]\n",
    "        correlation, p_val = spearmanr(train[var1].dropna(), train[var2].dropna())\n",
    "        print(\"{var1} vs {var2}:\")\n",
    "        print(\"Correlation coefficient: {correlation:.3f}\")\n",
    "        print(\"p-value: {p_val:.2e}\")\n",
    "        print(\"Significant: {'Yes' if p_val < 0.05 else 'No'}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeebcbf1",
   "metadata": {},
   "source": [
    "1. **Categorical Variables (Chi-square tests)**:\n",
    "   - These tests reveal if the distribution of categories (like conditioning intensity or graft type) differs significantly across race groups\n",
    "   - A significant p-value (< 0.05) indicates that the distribution is not independent of race\n",
    "\n",
    "2. **Numerical Variables (ANOVA tests)**:\n",
    "   - These tests show if there are significant differences in means across race groups\n",
    "   - For significant differences, Tukey's HSD test shows exactly which race groups differ\n",
    "   - This is particularly important for variables like age and comorbidity scores\n",
    "\n",
    "3. **Correlation Analysis**:\n",
    "   - Spearman correlation tests reveal relationships between numerical variables\n",
    "   - This helps understand which features might be redundant or complementary\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e1ce9",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5c486d",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate missing values percentage\n",
    "missing_values = train.isnull().sum() / len(train) * 100\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "\n",
    "# Generate a color gradient from dark blue (low missing) to red (high missing)\n",
    "colors = sns.color_palette(\"coolwarm\", len(missing_values))[::-1]\n",
    "\n",
    "# Plot missing values with updated formatting\n",
    "plt.figure(figsize=(14, 10))\n",
    "bars = sns.barplot(y=missing_values.index, x=missing_values.values, palette=colors, hue=missing_values.index)\n",
    "\n",
    "# Set x-axis to always go up to 100%\n",
    "plt.xlim(0, 100)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Percentage of Missing Values\", fontsize=14)\n",
    "plt.ylabel(\"Feature Name\", fontsize=14)\n",
    "plt.title(\"Missing Values Percentage per Feature\", fontsize=16)\n",
    "\n",
    "# Rotate x-axis ticks for better visibility\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12, rotation=0)\n",
    "\n",
    "# Annotate each bar with its missing percentage\n",
    "for bar, value in zip(bars.patches, missing_values.values):\n",
    "    plt.text(bar.get_width() + 2, bar.get_y() + bar.get_height()/2, \n",
    "             f\"{value:.1f}%\", ha='left', va='center', fontsize=12, color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaf5b27",
   "metadata": {},
   "source": [
    "Some features are incomplete. For example, we see that `tce_match` has a lot of missing values!  \n",
    "*(TCE Match refers to a specific type of immune match between donors and recipients in HCT transplants)*  \n",
    "\n",
    "What should we do?\n",
    "\n",
    "We have three options:\n",
    "1. Fill in (impute) the missing values using the mean, median, or mode.\n",
    "2. Drop the feature if it’s mostly empty.\n",
    "3. Treat missing values as a separate category - sometimes missing data itself carries meaning!\n",
    "\n",
    "For now, at least for the categorical features, we choose the third option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e82cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = list(train.select_dtypes(object).columns)\n",
    "train[cat_features] = train[cat_features].astype(str).astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da90dd18",
   "metadata": {},
   "source": [
    "Let's review the missing values now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67d19838",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate missing values percentage\n",
    "missing_values = train.isnull().sum() / len(train) * 100\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "\n",
    "# Generate a color gradient from dark blue (low missing) to red (high missing)\n",
    "colors = sns.color_palette(\"coolwarm\", len(missing_values))[::-1]\n",
    "\n",
    "# Plot missing values with updated formatting\n",
    "plt.figure(figsize=(14, 10))\n",
    "bars = sns.barplot(y=missing_values.index, x=missing_values.values, palette=colors, hue=missing_values.index)\n",
    "\n",
    "# Set x-axis to always go up to 100%\n",
    "plt.xlim(0, 100)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Percentage of Missing Values\", fontsize=14)\n",
    "plt.ylabel(\"Feature Name\", fontsize=14)\n",
    "plt.title(\"Missing Values Percentage per Feature\", fontsize=16)\n",
    "\n",
    "# Rotate x-axis ticks for better visibility\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12, rotation=0)\n",
    "\n",
    "# Annotate each bar with its missing percentage\n",
    "for bar, value in zip(bars.patches, missing_values.values):\n",
    "    plt.text(bar.get_width() + 2, bar.get_y() + bar.get_height()/2, \n",
    "             f\"{value:.1f}%\", ha='left', va='center', fontsize=12, color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7039399",
   "metadata": {},
   "source": [
    "Much better!  \n",
    "Now our models will be able to handle missing values for categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4553a9b",
   "metadata": {
    "papermill": {
     "duration": 0.009005,
     "end_time": "2024-12-22T18:02:40.719525",
     "exception": false,
     "start_time": "2024-12-22T18:02:40.710520",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The weirdness of the age distribution\n",
    "\n",
    "There are only two features with continuous data: donor age and patient age.  \n",
    "The patient age histogram shows that the patient age distribution has five modes.  \n",
    "Such a distribution is highly unnatural — it must be an artefact of the synthetic data generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cce1b5",
   "metadata": {},
   "source": [
    "Since this dataset was synthetically generated, the age distribution does not fully reflect real-world patient demographics. This is crucial to consider when interpreting model results, as certain patterns may be artifacts of data preprocessing rather than genuine clinical insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eee59fb6",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-22T18:02:40.739929Z",
     "iopub.status.busy": "2024-12-22T18:02:40.739524Z",
     "iopub.status.idle": "2024-12-22T18:02:41.627171Z",
     "shell.execute_reply": "2024-12-22T18:02:41.626072Z"
    },
    "papermill": {
     "duration": 0.900699,
     "end_time": "2024-12-22T18:02:41.629524",
     "exception": false,
     "start_time": "2024-12-22T18:02:40.728825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train.donor_age, bins=50, color='skyblue')\n",
    "plt.title('Donor age histogram')\n",
    "plt.xlabel('donor_age')\n",
    "plt.ylabel('count')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Patient age histogram')\n",
    "plt.hist(train.age_at_hct, bins=50, color='skyblue')\n",
    "plt.xlabel('age_at_hct')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95030d2c",
   "metadata": {
    "papermill": {
     "duration": 0.010833,
     "end_time": "2024-12-22T18:02:41.650540",
     "exception": false,
     "start_time": "2024-12-22T18:02:41.639707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Our first thought was that different race groups had different modes, but the patient age distribution has the same five modes in every race group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bd2ca6b",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-22T18:02:41.672843Z",
     "iopub.status.busy": "2024-12-22T18:02:41.672372Z",
     "iopub.status.idle": "2024-12-22T18:02:44.011912Z",
     "shell.execute_reply": "2024-12-22T18:02:44.010626Z"
    },
    "papermill": {
     "duration": 2.353577,
     "end_time": "2024-12-22T18:02:44.014132",
     "exception": false,
     "start_time": "2024-12-22T18:02:41.660555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, axs = plt.subplots(3, 2, sharex=True, sharey=True, figsize=(12, 9))\n",
    "for race_group, ax in zip(race_groups, axs.ravel()):\n",
    "    ax.hist(train.age_at_hct[train.race_group == race_group],\n",
    "            bins=np.linspace(0, 74, 38),\n",
    "            color='skyblue')\n",
    "    ax.set_title(f'Patient age histogram for {race_group}')\n",
    "    ax.set_xlabel('age_at_hct')\n",
    "    ax.set_ylabel('count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa30e3e1",
   "metadata": {
    "papermill": {
     "duration": 0.011138,
     "end_time": "2024-12-22T18:02:44.036821",
     "exception": false,
     "start_time": "2024-12-22T18:02:44.025683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Even stranger: The age of 0.044 years (i.e., 16 days) occurs 1005 times in the training dataset, whereas every other age occurs at most six times.  \n",
    "Is hematopoietic cell transplantation a treatment which is often done to newborns? Possible. But we can't believe that these babies are all treated exactly when they are 16 days old. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b12a485c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T18:02:44.060653Z",
     "iopub.status.busy": "2024-12-22T18:02:44.060234Z",
     "iopub.status.idle": "2024-12-22T18:02:44.077112Z",
     "shell.execute_reply": "2024-12-22T18:02:44.075869Z"
    },
    "papermill": {
     "duration": 0.031678,
     "end_time": "2024-12-22T18:02:44.079417",
     "exception": false,
     "start_time": "2024-12-22T18:02:44.047739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.age_at_hct.value_counts().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b634669",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 0.010575,
     "end_time": "2024-12-22T18:02:44.100959",
     "exception": false,
     "start_time": "2024-12-22T18:02:44.090384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The prediction target\n",
    "\n",
    "The prediction target consists of two parts:\n",
    "1. `efs_time`, always positive, is a time, measured in months.\n",
    "2. `efs`, always zero or one, indicates the presence or absence of an event:\n",
    "   - `efs=1` means \"patient was without event for **exactly** `efs_time`.\n",
    "   - `efs=0` means \"patient was without event for **at least** `efs_time`\"\n",
    "\n",
    "This situation is called \"censored data\": Samples of which we know the time of event are uncensored, and if we only know a lower bound for the time of event, the sample is (right-)censored.\n",
    "\n",
    "**Censoring is the main reason that this competition has a special metric and that we need special models.**  \n",
    "The competition is a regression task, but we know y_true for only half the samples.  \n",
    "For the other (censored) half, all we know is lower bounds for y_true.  \n",
    "One cannot compute a squared error based on `y_true > 100 and y_pred == 120`. RMSE and similar metrics cannot deal with that.\n",
    "\n",
    "Weird thing to note, `efs_time` is a float with three digits after the decimal point.  \n",
    "Even though it is measured in months, the recorded numbers don't really align to days for any month.  \n",
    "We doubt that possible events of a patient are recorded with a more exact unit than a day.  \n",
    "So this might also be a case caused by the synthetic data generation.\n",
    "\n",
    "A histogram of the target values shows that half the patients have an event within 20 months after the transplantation.  \n",
    "The other half, without event for the first 20 months, has a high probability of not having an event for much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f2e4bb7",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-22T18:02:44.124983Z",
     "iopub.status.busy": "2024-12-22T18:02:44.124524Z",
     "iopub.status.idle": "2024-12-22T18:02:44.463381Z",
     "shell.execute_reply": "2024-12-22T18:02:44.461985Z"
    },
    "papermill": {
     "duration": 0.353716,
     "end_time": "2024-12-22T18:02:44.465861",
     "exception": false,
     "start_time": "2024-12-22T18:02:44.112145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.hist(train.efs_time[train.efs == 0], bins=np.linspace(0, 160, 41), label='efs=0: patient has no event for at least this time', alpha=0.5)\n",
    "plt.hist(train.efs_time[train.efs == 1], bins=np.linspace(0, 160, 41), label='efs=1: patient has event at this time', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('efs_time')\n",
    "plt.ylabel('count')\n",
    "plt.title('Target histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b54012",
   "metadata": {
    "papermill": {
     "duration": 0.011379,
     "end_time": "2024-12-22T18:02:44.488932",
     "exception": false,
     "start_time": "2024-12-22T18:02:44.477553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Survival function and cumulative hazard function\n",
    "\n",
    "The survival function shows how many patients have no event for an amount of time ([Kaplan–Meier estimator](https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator)).  \n",
    "At month 0, 100% of the patients have no event. At month 20, only 40% - 60% remain without event, depending on their race group.  \n",
    "Patients with \"more than one race\" have the highest probability of event-free survival, while whites have the lowest.\n",
    "\n",
    "The cumulative hazard is another representation of the same facts, it corresponds to the negative logarithm of the survival function ([Nelson–Aalen estimator](https://en.wikipedia.org/wiki/Nelson%E2%80%93Aalen_estimator))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2244279",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-22T18:02:44.514200Z",
     "iopub.status.busy": "2024-12-22T18:02:44.513445Z",
     "iopub.status.idle": "2024-12-22T18:02:45.567175Z",
     "shell.execute_reply": "2024-12-22T18:02:45.565789Z"
    },
    "papermill": {
     "duration": 1.069426,
     "end_time": "2024-12-22T18:02:45.570012",
     "exception": false,
     "start_time": "2024-12-22T18:02:44.500586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def survival_function(df):\n",
    "    survival_df = df[['efs', 'efs_time']].groupby('efs_time').agg(['size', 'sum']).droplevel(0, axis=1).astype(int)\n",
    "    survival_df['n_at_risk'] = survival_df['size'].sum() - survival_df['size'].shift().fillna(0).cumsum().astype(int)\n",
    "    hazard = survival_df['sum'] / survival_df['n_at_risk'] \n",
    "    survival_df['cumulative_hazard'] = np.cumsum(hazard) # nelson_aalen_estimator\n",
    "    survival_df['survival_probability'] = (1 - hazard).cumprod() # kaplan_meier_estimator\n",
    "    return survival_df\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "survival_df = survival_function(train)\n",
    "plt.step(survival_df.index, survival_df['survival_probability'], c='k', where=\"post\", label='[Overall]')\n",
    "plt.xlabel('efs_time')\n",
    "for race_group in race_groups:\n",
    "    subset = train.query('race_group == @race_group')\n",
    "    survival_df = survival_function(subset)\n",
    "    plt.step(survival_df.index, survival_df['survival_probability'], where=\"post\", label=race_group)\n",
    "plt.xlabel('efs_time')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Survival function (Kaplan–Meier) by race group')\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(xmax=1, decimals=0)) # percent of xmax\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "survival_df = survival_function(train)\n",
    "plt.step(survival_df.index, survival_df['cumulative_hazard'], c='k', where=\"post\", label='[Overall]')\n",
    "plt.xlabel('efs_time')\n",
    "for race_group in race_groups:\n",
    "    subset = train.query('race_group == @race_group')\n",
    "    survival_df = survival_function(subset)\n",
    "    plt.step(survival_df.index, survival_df['cumulative_hazard'], where=\"post\", label=race_group)\n",
    "plt.xlabel('efs_time')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Cumulative hazard (Nelson–Aalen) by race group')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37c2e3",
   "metadata": {},
   "source": [
    "### Is the Difference in Survival Significant?  \n",
    "So far, we've seen that survival probabilities **vary by race group**, but are these differences **statistically significant**?  \n",
    "To answer this, we'll use the **Log-Rank Test**, a statistical test designed to compare survival distributions between groups.  \n",
    "\n",
    "If the p-value is **less than 0.05**, it means the difference is **statistically significant**—one group truly has a better survival outcome than the other. If not, any differences we see could be due to chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fda992e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "# Select two race groups for comparison\n",
    "race_group_1 = train[train[\"race_group\"] == \"White\"]\n",
    "race_group_2 = train[train[\"race_group\"] == \"More than one race\"]\n",
    "\n",
    "# Perform Log-Rank Test\n",
    "log_rank_result = logrank_test(\n",
    "    race_group_1[\"efs_time\"], race_group_2[\"efs_time\"],\n",
    "    event_observed_A=race_group_1[\"efs\"],\n",
    "    event_observed_B=race_group_2[\"efs\"]\n",
    ")\n",
    "\n",
    "# Print p-value to check statistical significance\n",
    "print(f\"Log-Rank Test p-value: {log_rank_result.p_value}\")\n",
    "\n",
    "# Interpretation of results\n",
    "if log_rank_result.p_value < 0.05:\n",
    "    print(\"✅ Significant survival difference! One race group has a statistically different survival rate.\")\n",
    "else:\n",
    "    print(\"❌ No significant difference in survival between these race groups.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb25d8",
   "metadata": {},
   "source": [
    "Findings:\n",
    "- The p-value obtained from the test is 1.79e-49, which is extremely small (<< 0.05).\n",
    "-\tThis means that the difference in survival rates between the \"White\" group and the \"More than one race\" group is statistically significant.\n",
    "-\tIn other words, one of these groups experiences better survival outcomes than the other, and this difference is unlikely due to random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2575a",
   "metadata": {},
   "source": [
    "### Feature correlation\n",
    "Sometimes, two features are so similar that they provide the same information.  \n",
    "If this happens, we might remove one to make our model simpler.\n",
    "\n",
    "Let's explore the relationships between numerical features.  \n",
    "Correlations can help us identify redundant features, where one variable provides almost the same information as another.  \n",
    "High correlation (close to 1 or -1) means we might consider removing one of the features to avoid redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "704af5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr_matrix = train.corr(numeric_only=True) # Only numerical columns\n",
    "sns.heatmap(corr_matrix, cmap=\"coolwarm\", annot=False, linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd07a44",
   "metadata": {},
   "source": [
    "1. \tSome features have a very high correlation:  \n",
    "\tMany HLA match features (`hla_match_*`, `hla_high_res_*`) are strongly correlated.  \n",
    "\tThis makes sense! They’re all measuring similar compatibility factors between donors and patients.  \n",
    "\tDo we really need all of them? Maybe not! We might remove some duplicates to keep things simple.\n",
    "\n",
    "2. \tSurvival time (`efs_time`) seems Hard to predict:  \n",
    "\t`efs_time` doesn’t have strong correlations with most features.  \n",
    "\tThis tells us that survival time depends on many small factors rather than one big predictor.\n",
    "\n",
    "3.  Age and donor age:  \n",
    "\tPatients (`age_at_hct`) and their donors (`donor_age`) show some relationship, but it's not very strong.\n",
    "\n",
    "4. \tHealth scores:  \n",
    "\tComorbidity score and Karnofsky score have some level of correlation with each other.  \n",
    "\tThis makes sense! A patient with more health issues (high comorbidity) probably has a lower Karnofsky score (worse physical condition)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316330c5",
   "metadata": {},
   "source": [
    "Important Consideration Before Model Evaluation:\n",
    "\n",
    "Since the dataset is synthetic, some of the patterns our models learn may not generalize to real-world patient outcomes. This is especially important when interpreting survival probabilities, as factors influencing patient outcomes in this dataset may differ from those in actual clinical settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da203a71",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "This competition is about equity in the predictions. This means that we score the predictions per race group and then derive the final score from these six sub-scores.  \n",
    "As the official implementation of the competition metric doesn't output the scores per race group, we've written our own implementation, which gives more transparency.\n",
    "\n",
    "We also show the scores recieved for each fold in the cross validation.\n",
    "The overall score is the mean of all of the fold's scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e8174f9",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-22T18:02:45.625162Z",
     "iopub.status.busy": "2024-12-22T18:02:45.624741Z",
     "iopub.status.idle": "2024-12-22T18:02:45.636026Z",
     "shell.execute_reply": "2024-12-22T18:02:45.634840Z"
    },
    "papermill": {
     "duration": 0.028146,
     "end_time": "2024-12-22T18:02:45.638279",
     "exception": false,
     "start_time": "2024-12-22T18:02:45.610133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5 ,shuffle=True, random_state=1)\n",
    "all_scores = {}\n",
    "X_va = {}\n",
    "idx_va = {}\n",
    "\n",
    "def evaluate_fold(y_va_pred, fold):\n",
    "    \"\"\"Compute and print the metrics (concordance index) per race group for a single fold.\n",
    "\n",
    "    Global variables:\n",
    "    - train, X_va, idx_va\n",
    "    - The metrics are saved in the global list all_scores.\n",
    "    \"\"\"\n",
    "    metric_list = []\n",
    "    for race in race_groups:\n",
    "        mask = X_va.race_group.values == race\n",
    "        c_index_race = concordance_index(\n",
    "            train.efs_time.iloc[idx_va][mask],\n",
    "            - y_va_pred[mask],\n",
    "            train.efs.iloc[idx_va][mask]\n",
    "        )\n",
    "        # print(f\"# {race:42} {c_index_race:.3f}\")\n",
    "        metric_list.append(c_index_race)\n",
    "    fold_score = np.mean(metric_list) - np.sqrt(np.var(metric_list))\n",
    "    print(f\"# Total fold {fold}:{' ':29} {fold_score:.3f} mean={np.mean(metric_list):.3f} std={np.std(metric_list):.3f}\")\n",
    "    all_scores.append(metric_list)\n",
    "\n",
    "def display_overall(label):\n",
    "    \"\"\"Compute and print the overall metrics (concordance index)\"\"\"\n",
    "    df = pd.DataFrame(all_scores, columns=race_groups)\n",
    "    df['mean'] = df[race_groups].mean(axis=1)\n",
    "    df['std'] = np.std(df[race_groups], axis=1)\n",
    "    df['score'] = df['mean'] - df['std']\n",
    "    df = df.T\n",
    "    df['Overall'] = df.mean(axis=1)\n",
    "    temp = df.drop(index=['std']).values\n",
    "    print(f\"# Overall:                                   {df.loc['score', 'Overall']:.3f} {label}\")\n",
    "    all_model_scores[label] = df.loc['score', 'Overall']\n",
    "    display(df\n",
    "            .iloc[:len(race_groups)]\n",
    "            .style\n",
    "            .format(precision=3)\n",
    "            .background_gradient(axis=None, vmin=temp.min(), vmax=temp.max(), cmap=\"cool\")\n",
    "            .concat(df.iloc[len(race_groups):].style.format(precision=3))\n",
    "           )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c929c81",
   "metadata": {},
   "source": [
    "# Cox proportional hazards model\n",
    "One method for survival analysis, is the [Cox proportional hazards model](https://en.wikipedia.org/wiki/Proportional_hazards_model).  \n",
    "Which is implemented in XGBoost and in CatBoost.\n",
    "\n",
    "Let's try the first method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723b31ff",
   "metadata": {},
   "source": [
    "### XGBoost Cox proportional hazards model\n",
    "Cox proportional hazards model with XGBoost.  \n",
    "This model expects that the two target columns be combined into one (`y = np.where(train.efs == 1, train.efs_time, -train.efs_time)`, negative target values are considered right censored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e08d37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "y = np.where(train.efs == 1, train.efs_time, -train.efs_time)\n",
    "all_scores = []\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(train, train.race_group)):\n",
    "    X_tr = train.iloc[idx_tr][features]\n",
    "    X_va = train.iloc[idx_va][features]\n",
    "    y_tr = y[idx_tr]\n",
    "    \n",
    "    xgb_cox_params = {'objective': 'survival:cox', 'grow_policy': 'depthwise', \n",
    "                      'n_estimators': 700, 'learning_rate': 0.0254, 'max_depth': 8, \n",
    "                      'reg_lambda': 0.116, 'reg_alpha': 0.139, 'min_child_weight': 23.8,\n",
    "                      'colsample_bytree': 0.59, 'subsample': 0.7, 'tree_method': 'hist',\n",
    "                      'enable_categorical': True}\n",
    "    model = xgboost.XGBRegressor(**xgb_cox_params)\n",
    "    model.fit(X_tr, y_tr) # negative values are considered right censored\n",
    "    y_va_pred = model.predict(X_va) # predicts hazard factor\n",
    "    evaluate_fold(y_va_pred, fold)\n",
    "display_overall('Cox Proportional Hazards XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f39221d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T18:02:45.667591Z",
     "iopub.status.busy": "2024-12-22T18:02:45.667178Z",
     "iopub.status.idle": "2024-12-22T18:03:24.422973Z",
     "shell.execute_reply": "2024-12-22T18:03:24.421350Z"
    },
    "papermill": {
     "duration": 38.773328,
     "end_time": "2024-12-22T18:03:24.425069",
     "exception": false,
     "start_time": "2024-12-22T18:02:45.651741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "y = np.where(train.efs == 1, train.efs_time, -train.efs_time)\n",
    "all_scores = []\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(train, train.race_group)):\n",
    "    X_tr = train.iloc[idx_tr][features]\n",
    "    X_va = train.iloc[idx_va][features]\n",
    "    y_tr = y[idx_tr]\n",
    "    \n",
    "    xgb_cox_params = {'objective': 'survival:cox', 'grow_policy': 'depthwise', \n",
    "                      'n_estimators': 700, 'learning_rate': 0.0254, 'max_depth': 8, \n",
    "                      'reg_lambda': 0.116, 'reg_alpha': 0.139, 'min_child_weight': 23.8,\n",
    "                      'colsample_bytree': 0.59, 'subsample': 0.7, 'tree_method': 'hist',\n",
    "                      'enable_categorical': True}\n",
    "    model = xgboost.XGBRegressor(**xgb_cox_params)\n",
    "    model.fit(X_tr, y_tr) # negative values are considered right censored\n",
    "    y_va_pred = model.predict(X_va) # predicts hazard factor\n",
    "    evaluate_fold(y_va_pred, fold)\n",
    "display_overall('Cox Proportional Hazards XGBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4094a",
   "metadata": {},
   "source": [
    "### Catboost Cox proportional hazards model\n",
    "Cox proportional hazards model with CatBoost.  \n",
    "This model expects the targets in the same format as the XGBoost Cox model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "906d05e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T18:03:24.455690Z",
     "iopub.status.busy": "2024-12-22T18:03:24.455129Z",
     "iopub.status.idle": "2024-12-22T18:11:05.164261Z",
     "shell.execute_reply": "2024-12-22T18:11:05.163053Z"
    },
    "papermill": {
     "duration": 460.742381,
     "end_time": "2024-12-22T18:11:05.181948",
     "exception": false,
     "start_time": "2024-12-22T18:03:24.439567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "y = np.where(train.efs == 1, train.efs_time, -train.efs_time)\n",
    "all_scores = []\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(train, train.race_group)):\n",
    "    X_tr = train.iloc[idx_tr][features]\n",
    "    X_va = train.iloc[idx_va][features]\n",
    "    y_tr = y[idx_tr]\n",
    "    \n",
    "    cb_cox_params = {'loss_function': 'Cox', 'grow_policy': 'SymmetricTree',\n",
    "                     'n_estimators': 800, 'learning_rate': 0.092, 'l2_leaf_reg': 2.5,\n",
    "                     'max_depth': 7, 'colsample_bylevel': 0.84, 'subsample': 0.9, \n",
    "                     'random_strength': 0.8, 'verbose': False}\n",
    "    \n",
    "    model = catboost.CatBoostRegressor(**cb_cox_params, cat_features=cat_features)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_va_pred = model.predict(X_va) # predicts log of hazard factor\n",
    "    evaluate_fold(y_va_pred, fold)\n",
    "display_overall('Cox Proportional Hazards CatBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d6168",
   "metadata": {},
   "source": [
    "# Accelerated failure time model\n",
    "Another method for survival analysis, is the [Accelerated failure time model](https://en.wikipedia.org/wiki/Accelerated_failure_time_model).  \n",
    "Which is also implemented in XGBoost and in CatBoost.\n",
    "\n",
    "We shall try it too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023499f",
   "metadata": {},
   "source": [
    "### XGBoost Accelerated failure time model\n",
    "Accelerated failure time model with XGBoost.  \n",
    "This model expects the lower and upper bounds for the target in a special form in a DMatrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e38d05b",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-22T18:11:05.213633Z",
     "iopub.status.busy": "2024-12-22T18:11:05.213232Z",
     "iopub.status.idle": "2024-12-22T18:11:15.827031Z",
     "shell.execute_reply": "2024-12-22T18:11:15.825813Z"
    },
    "papermill": {
     "duration": 10.632798,
     "end_time": "2024-12-22T18:11:15.829744",
     "exception": false,
     "start_time": "2024-12-22T18:11:05.196946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "all_scores = []\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(train, train.race_group)):\n",
    "    X_tr = train.iloc[idx_tr][features]\n",
    "    X_va = train.iloc[idx_va][features]\n",
    "    d_tr = xgboost.DMatrix(X_tr, enable_categorical=True)\n",
    "    d_tr.set_float_info('label_lower_bound', train.efs_time.iloc[idx_tr])\n",
    "    d_tr.set_float_info('label_upper_bound', np.where(train.efs.iloc[idx_tr] == 0, np.inf, train.efs_time.iloc[idx_tr]))\n",
    "    d_va = xgboost.DMatrix(X_va, enable_categorical=True)\n",
    "    d_va.set_float_info('label_lower_bound', train.efs_time.iloc[idx_va])\n",
    "    d_va.set_float_info('label_upper_bound', np.where(train.efs.iloc[idx_va] == 0, np.inf, train.efs_time.iloc[idx_va]))\n",
    "    xgboost_aft_params = {'learning_rate': 0.08, 'max_depth': 4, 'reg_lambda': 3, 'aft_loss_distribution_scale': 0.9,\n",
    "                          'reg_alpha': 0.24, 'gamma': 0.033, 'min_child_weight': 82.58861553592878,\n",
    "                          'colsample_bytree': 0.5662198438953138, 'max_bin': 53, 'subsample': 0.7456329821182728, \n",
    "                          'objective': 'survival:aft', 'grow_policy': 'depthwise', 'tree_method': 'hist',\n",
    "                          'aft_loss_distribution': 'normal'}\n",
    "    bst = xgboost.train(xgboost_aft_params,\n",
    "                        d_tr,\n",
    "                        num_boost_round=300,\n",
    "                       )\n",
    "    y_va_pred = - bst.predict(d_va) # model predicts time of event\n",
    "    evaluate_fold(y_va_pred, fold)\n",
    "display_overall('Accelerated Failure Time XGBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a2435",
   "metadata": {},
   "source": [
    "### CatBoost Accelerated failure time model\n",
    "Accelerated failure time model with CatBoost.  \n",
    "This model expects the lower and upper bounds for the target in the form of a two-column array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6002545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T18:11:15.862148Z",
     "iopub.status.busy": "2024-12-22T18:11:15.861714Z",
     "iopub.status.idle": "2024-12-22T18:16:58.390321Z",
     "shell.execute_reply": "2024-12-22T18:16:58.389061Z"
    },
    "papermill": {
     "duration": 342.564984,
     "end_time": "2024-12-22T18:16:58.410233",
     "exception": false,
     "start_time": "2024-12-22T18:11:15.845249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "y = np.column_stack([train.efs_time,\n",
    "                     np.where(train.efs == 1, train.efs_time, -1)])\n",
    "all_scores = []\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(train, train.race_group)):\n",
    "    X_tr = train.iloc[idx_tr][features]\n",
    "    X_va = train.iloc[idx_va][features]\n",
    "    y_tr = y[idx_tr]\n",
    "    cb_aft_params = {'loss_function': 'SurvivalAft', 'grow_policy': 'SymmetricTree', \n",
    "                     'n_estimators': 800, 'learning_rate': 0.066, 'l2_leaf_reg': 4.4,\n",
    "                     'max_depth': 5, 'colsample_bylevel': 0.776, 'random_strength': 0.9, \n",
    "                     'verbose': False}\n",
    "    model = catboost.CatBoostRegressor(**cb_aft_params, cat_features=cat_features)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_va_pred = - model.predict(X_va) # model predicts log of time of event\n",
    "    evaluate_fold(y_va_pred, fold)\n",
    "display_overall('Accelerated Failure Time CatBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ff9f2",
   "metadata": {
    "_kg_hide-input": false,
    "papermill": {
     "duration": 0.017436,
     "end_time": "2024-12-22T18:16:58.445074",
     "exception": false,
     "start_time": "2024-12-22T18:16:58.427638",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Target transformation models and regression with mean squared error\n",
    "\n",
    "The competition task can be interpreted as predicting the order of events for the patients.  \n",
    "Who has an event first? Who second? ... Who has an event last? and who survives without an event at all?  \n",
    "With a suitable target transformation, we can apply the usual regression algorithms which optimize MSE or similar metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d57ce",
   "metadata": {},
   "source": [
    "### Target transformations\n",
    "As we saw in the EDA, patients with a known event, mostly have an `efs_time` between 0 and 15, whereas most unkowns have an `efs_time` between 15 and 160.  \n",
    "This distribution is problematic for regression models.  \n",
    "We want predictions to have high discriminative power for the patients who have a known event, but we don't need to distinguish between unkowns.  \n",
    "We can achieve this result by stretching the range of the patients with a known event and compressing the range of the unkowns.\n",
    "\n",
    "Let's create such a transformation to use for our model:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37fa4944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_survival_probability(time, event):\n",
    "    \"\"\"Transform the target by stretching the range of eventful efs_times and compressing the range of event_free efs_times\"\"\"\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(time, event)\n",
    "    return kmf.survival_function_at_times(time).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e6e76",
   "metadata": {},
   "source": [
    "The next diagram shows how a typical target transformation stretches and compresses the ranges.  \n",
    "We use the transformation above as the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63da9260",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-22T18:16:58.481060Z",
     "iopub.status.busy": "2024-12-22T18:16:58.480662Z",
     "iopub.status.idle": "2024-12-22T18:16:59.853803Z",
     "shell.execute_reply": "2024-12-22T18:16:59.852662Z"
    },
    "papermill": {
     "duration": 1.393808,
     "end_time": "2024-12-22T18:16:59.856194",
     "exception": false,
     "start_time": "2024-12-22T18:16:58.462386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_quantile = transform_survival_probability(time=train.efs_time, event=train.efs)\n",
    "survival_df = survival_function(train)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10), dpi=80)\n",
    "\n",
    "axs[0, 0].hist(train.efs_time[train.efs == 0], bins=np.linspace(0, 160, 41), label='efs=0: patient has no event for this time', alpha=0.5)\n",
    "axs[0, 0].hist(train.efs_time[train.efs == 1], bins=np.linspace(0, 160, 41), label='efs=1: patient has event at this time', alpha=0.5)\n",
    "axs[0, 0].legend()\n",
    "axs[0, 0].set_xlabel('efs_time')\n",
    "axs[0, 0].set_ylabel('count')\n",
    "axs[0, 0].set_title('Original target histogram')\n",
    "\n",
    "axs[0, 1].set_axis_off()\n",
    "\n",
    "axs[1, 0].step(survival_df.index, survival_df['survival_probability'], c='k', lw=3, where=\"post\", label='[Overall]')\n",
    "axs[1, 0].set_xlabel('efs_time')\n",
    "axs[1, 0].set_ylabel(\"quantile\")\n",
    "axs[1, 0].set_title(\"Survival function\")\n",
    "axs[1, 0].yaxis.set_major_formatter(PercentFormatter(xmax=1, decimals=0))\n",
    "\n",
    "axs[1, 1].hist(y_quantile[train.efs==0], bins=100, label=\"efs=0\", orientation=u'horizontal', alpha=0.5)\n",
    "axs[1, 1].hist(y_quantile[train.efs==1], bins=100, label=\"efs=1\", orientation=u'horizontal', alpha=0.5)\n",
    "axs[1, 1].legend()\n",
    "axs[1, 1].set_ylabel(\"quantile\")\n",
    "axs[1, 1].set_xlabel(\"count\")\n",
    "axs[1, 1].set_title(\"Transformed target histogram (sideways)\")\n",
    "axs[1, 1].yaxis.set_major_formatter(PercentFormatter(xmax=1, decimals=0))\n",
    "\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "ax.set_axis_off()\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "fig.add_axes(ax)\n",
    "ax.arrow(0.2, 0.55, 0, -0.47, length_includes_head=True, width=0.002, color=plt.get_cmap('tab10')(0), alpha=0.5, head_width=0.02, head_length=0.02)\n",
    "ax.arrow(0.2, 0.082, 0.37, 0, length_includes_head=True, width=0.002, color=plt.get_cmap('tab10')(0), alpha=0.5, head_width=0.02, head_length=0.02)\n",
    "ax.arrow(0.12, 0.55, 0, -0.3, length_includes_head=True, width=0.002, color=plt.get_cmap('tab10')(1), alpha=0.5, head_width=0.02, head_length=0.02)\n",
    "ax.arrow(0.12, 0.25, 0.45, 0, length_includes_head=True, width=0.002, color=plt.get_cmap('tab10')(1), alpha=0.5, head_width=0.02, head_length=0.02)\n",
    "\n",
    "plt.suptitle('Transforming the target', y=0.99, size=20)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145a365",
   "metadata": {
    "papermill": {
     "duration": 0.017552,
     "end_time": "2024-12-22T18:16:59.890965",
     "exception": false,
     "start_time": "2024-12-22T18:16:59.873413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will add 4 more possible target transformations to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2dd3ed4",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2024-12-22T18:16:59.929762Z",
     "iopub.status.busy": "2024-12-22T18:16:59.929394Z",
     "iopub.status.idle": "2024-12-22T18:16:59.940494Z",
     "shell.execute_reply": "2024-12-22T18:16:59.939082Z"
    },
    "papermill": {
     "duration": 0.032891,
     "end_time": "2024-12-22T18:16:59.942960",
     "exception": false,
     "start_time": "2024-12-22T18:16:59.910069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_partial_hazard(time, event):\n",
    "    \"\"\"Transform the target by stretching the range of eventful efs_times and compressing the range of event_free efs_times\"\"\"\n",
    "    data = pd.DataFrame({'efs_time': time, 'efs': event, 'time': time, 'event': event})\n",
    "    cph = CoxPHFitter()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        cph.fit(data, duration_col='time', event_col='event')\n",
    "    return cph.predict_partial_hazard(data)\n",
    "\n",
    "def transform_separate(time, event):\n",
    "    \"\"\"Transform the target by separating events from non-events\"\"\"\n",
    "    transformed = time.values.copy()\n",
    "    mx = transformed[event == 1].max() # last patient that has a known event\n",
    "    mn = transformed[event == 0].min() # first patient that does not have a known event\n",
    "    transformed[event == 0] = time[event == 0] + mx - mn\n",
    "    transformed = rankdata(transformed)\n",
    "    transformed[event == 0] += len(transformed) // 2\n",
    "    transformed = transformed / transformed.max()\n",
    "    return - transformed\n",
    "\n",
    "def transform_rank_log(time, event):\n",
    "    \"\"\"Transform the target by stretching the range of eventful efs_times and compressing the range of event_free efs_times\"\"\"\n",
    "    transformed = time.values.copy()\n",
    "    mx = transformed[event == 1].max() # last patient that has a known event\n",
    "    mn = transformed[event == 0].min() # first patient that does not have a known event\n",
    "    transformed[event == 0] = time[event == 0] + mx - mn\n",
    "    transformed = rankdata(transformed)\n",
    "    transformed[event == 0] += len(transformed) * 2\n",
    "    transformed = transformed / transformed.max()\n",
    "    transformed = np.log(transformed)\n",
    "    return - transformed\n",
    "\n",
    "def transform_quantile(time, event):\n",
    "    \"\"\"Transform the target by stretching the range of eventful efs_times and compressing the range of event_free efs_times\"\"\"\n",
    "    transformed = np.full(len(time), np.nan)\n",
    "    transformed_with_event = quantile_transform(- time[event == 1].values.reshape(-1, 1)).ravel()\n",
    "    transformed[event == 1] = transformed_with_event\n",
    "    transformed[event == 0] = transformed_with_event.min() - 0.3\n",
    "    return transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2e477",
   "metadata": {},
   "source": [
    "### XGBoost MSE loss with five different target transformations\n",
    "We now plot the histograms of the five possible transformations we have created, and then fit regression models with MSE loss to each of the transformed targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d982d61c",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-22T18:16:59.979958Z",
     "iopub.status.busy": "2024-12-22T18:16:59.979472Z",
     "iopub.status.idle": "2024-12-22T18:20:28.811749Z",
     "shell.execute_reply": "2024-12-22T18:20:28.810576Z"
    },
    "papermill": {
     "duration": 208.875628,
     "end_time": "2024-12-22T18:20:28.836348",
     "exception": false,
     "start_time": "2024-12-22T18:16:59.960720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "for transformation in [transform_survival_probability,\n",
    "                       transform_partial_hazard,\n",
    "                       transform_separate,\n",
    "                       transform_rank_log,\n",
    "                       transform_quantile,\n",
    "                      ]:\n",
    "    plt.figure(figsize=(6, 1.5))\n",
    "    target = transformation(time=train.efs_time, event=train.efs)\n",
    "    vmin, vmax = 1.09 * target.min() - 0.09 * target.max(), 1.09 * target.max() - 0.09 * target.min()\n",
    "    plt.hist(target[train.efs == 0], bins=np.linspace(vmin, vmax, 31), density=True, label='efs=0: patient has no event for this time', alpha=0.5)\n",
    "    plt.hist(target[train.efs == 1], bins=np.linspace(vmin, vmax, 31), density=True, label='efs=1: patient has event at this time', alpha=0.5)\n",
    "    plt.xlim(vmin, vmax)\n",
    "    plt.yticks([])\n",
    "    plt.title('Target histogram: ' + transformation.__name__)\n",
    "    plt.show()\n",
    "    \n",
    "    print(transformation.__name__)\n",
    "\n",
    "    all_scores = []\n",
    "    for fold, (idx_tr, idx_va) in enumerate(kf.split(train, train.race_group)):\n",
    "        X_tr = train.iloc[idx_tr][features]\n",
    "        X_va = train.iloc[idx_va][features]\n",
    "        y_tr = transformation(time=train.iloc[idx_tr].efs_time, event=train.iloc[idx_tr].efs)\n",
    "    \n",
    "        model = xgboost.XGBRegressor(\n",
    "            max_depth=3,  \n",
    "            colsample_bytree=0.5,  \n",
    "            subsample=0.8,  \n",
    "            n_estimators=2000,  \n",
    "            learning_rate=0.02,  \n",
    "            enable_categorical=True,\n",
    "            min_child_weight=80,\n",
    "        )\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_va_pred = model.predict(X_va) # predicts quantile\n",
    "        evaluate_fold(y_va_pred, fold)\n",
    "    display_overall(f'{transformation.__name__} XGBoost (MSE)')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da676a4",
   "metadata": {
    "papermill": {
     "duration": 0.021338,
     "end_time": "2024-12-22T18:20:28.879483",
     "exception": false,
     "start_time": "2024-12-22T18:20:28.858145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A linear model\n",
    "\n",
    "The linear model [CoxPHFitter](https://lifelines.readthedocs.io/en/latest/Survival%20Regression.html#cox-s-proportional-hazard-model) needs one-hot encoding and missing value imputation.  \n",
    "This is a Cox proportional hazards model with a linear implementation. This model expects time and event columns in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa4bac59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T18:20:28.926374Z",
     "iopub.status.busy": "2024-12-22T18:20:28.925939Z",
     "iopub.status.idle": "2024-12-22T18:22:57.824167Z",
     "shell.execute_reply": "2024-12-22T18:22:57.822851Z"
    },
    "papermill": {
     "duration": 148.925008,
     "end_time": "2024-12-22T18:22:57.826882",
     "exception": false,
     "start_time": "2024-12-22T18:20:28.901874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "all_scores = []\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(train, train.race_group)):\n",
    "    preproc = ColumnTransformer([('ohe', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), cat_features),\n",
    "                                ],\n",
    "                                remainder=SimpleImputer(strategy='median'),\n",
    "                                verbose_feature_names_out=False\n",
    "                               ).set_output(transform='pandas')\n",
    "    X_tr = preproc.fit_transform(train.iloc[idx_tr])\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        X_va = preproc.transform(train.iloc[idx_va])\n",
    "    model = CoxPHFitter(penalizer=.01)\n",
    "    feats = [f for f in X_tr.columns if f not in ['gvhd_proph_FK+- others(not MMF,MTX)']]\n",
    "    model.fit(X_tr[feats], duration_col='efs_time', event_col='efs')\n",
    "    # model.print_summary()\n",
    "    y_va_pred = model.predict_partial_hazard(X_va[feats])\n",
    "    X_va['race_group'] = train.race_group.iloc[idx_va]\n",
    "    evaluate_fold(y_va_pred, fold)\n",
    "display_overall('Cox Proportional Hazards Linear')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6edcbd",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2bd18",
   "metadata": {
    "papermill": {
     "duration": 0.022406,
     "end_time": "2024-12-22T18:22:57.917462",
     "exception": false,
     "start_time": "2024-12-22T18:22:57.895056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Final comparison\n",
    "\n",
    "For the time being, the gradient-boosted proportional hazard models (Cox regression, blue) and the transformed-target models (pink) win.  \n",
    "Among the target transformations, `transform_quantile` is best.  \n",
    "The AFT models (green) perhaps need more hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31dd11b1",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-22T18:22:57.964687Z",
     "iopub.status.busy": "2024-12-22T18:22:57.964259Z",
     "iopub.status.idle": "2024-12-22T18:22:58.263241Z",
     "shell.execute_reply": "2024-12-22T18:22:58.261636Z"
    },
    "papermill": {
     "duration": 0.325582,
     "end_time": "2024-12-22T18:22:58.265578",
     "exception": false,
     "start_time": "2024-12-22T18:22:57.939996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(all_model_scores, index=['score']).T\n",
    "result_df = result_df.sort_values('score', ascending=False)\n",
    "# with pd.option_context(\"display.precision\", 3): display(result_df)\n",
    "plt.figure(figsize=(6, len(result_df) * 0.4))\n",
    "\n",
    "color = np.where(result_df.index.str.contains('Proportional'),\n",
    "                 'cyan',\n",
    "                 np.where(result_df.index.str.contains('Accelerated'), 'lightgreen', \n",
    "                          'lightpink'))\n",
    "bars = plt.barh(np.arange(len(result_df)), result_df.score, color=color)\n",
    "plt.gca().bar_label(bars, fmt='%.3f')\n",
    "plt.yticks(np.arange(len(result_df)), result_df.index)\n",
    "plt.xlim(0.65, 0.68)\n",
    "plt.xticks([0.65, 0.66, 0.67, 0.68])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('CV score (higher is better)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e00e5b",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "As `transform_quantile` was our best model,  \n",
    "let's delve deeper into what features it cared about most:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dec4b155",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importances = []\n",
    "\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(train, train.race_group)):\n",
    "    X_tr = train.iloc[idx_tr][features]\n",
    "    X_va = train.iloc[idx_va][features]\n",
    "    y_tr = transform_quantile(time=train.iloc[idx_tr].efs_time, event=train.iloc[idx_tr].efs)\n",
    "\n",
    "    model = xgboost.XGBRegressor(\n",
    "        max_depth=3,  \n",
    "        colsample_bytree=0.5,  \n",
    "        subsample=0.8,  \n",
    "        n_estimators=2000,  \n",
    "        learning_rate=0.02,  \n",
    "        enable_categorical=True,\n",
    "        min_child_weight=80,\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Store feature importance\n",
    "    feature_importances.append(model.feature_importances_)\n",
    "\n",
    "# Convert to numpy array and compute mean importance\n",
    "feature_importances = np.array(feature_importances)\n",
    "mean_importance = feature_importances.mean(axis=0)\n",
    "\n",
    "# Create DataFrame for mean feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Importance\": mean_importance\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Normalize importance values for colormap\n",
    "norm = plt.Normalize(importance_df[\"Importance\"].min(), importance_df[\"Importance\"].max())\n",
    "colors = plt.cm.viridis(norm(importance_df[\"Importance\"]))\n",
    "\n",
    "plt.figure(figsize=(18, 15))\n",
    "bars = plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"], color=colors)\n",
    "\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"transform_quantile XGBoost (MSE) Feature Importance\")\n",
    "plt.gca().invert_yaxis()  # Flip features for better readability\n",
    "\n",
    "# Add labels to each bar\n",
    "for bar in bars:\n",
    "    plt.text(\n",
    "        bar.get_width() + 0.001,  # Position the text slightly to the right of the bar\n",
    "        bar.get_y() + bar.get_height() / 2,  # Center the text vertically\n",
    "        f\"{bar.get_width():.4f}\",  # Format the value\n",
    "        va=\"center\",  # Align vertically\n",
    "        ha=\"left\",  # Align horizontally\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62488fa6",
   "metadata": {},
   "source": [
    "These results make a lot of sense!  \n",
    "The `conditioning_intensity` describes how a patient's body was prepared for HCT.  \n",
    "Other top features include various scores (`dri_score`, `comorbidity_score`, etc.) that are given to patients to determine their health before HCT.  \n",
    "It fits that they contribute the most to the score as they reflect the physical state the patient was in before HCT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12771a53",
   "metadata": {},
   "source": [
    "### Race group inequality\n",
    "With most models, the Asian predictions get the highest scores (best concordance index) and the predictions for white patients get the lowest scores (worst concordance). \n",
    "\n",
    "As the competition's objective (equitability across diverse patient populations) rewards models with similar concordance scores for all six race groups,  \n",
    "a possible strategy could be to artificially make the predictions for Asian patients worse.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa4a142",
   "metadata": {},
   "source": [
    "### Working environment\n",
    "As we heard many disrecommendations for Kaggle notebooks as a work environment, we immediately skipped that option.\n",
    "\n",
    "First we tried Google collab, but it proved difficult to work in,  \n",
    "as the platform has bad intellisense, and no real realtime abillities.\n",
    "\n",
    "We decided to switch to a rather newer option, Deepnote.  \n",
    "It was easier to work there, due to having built in intellisense, realtime collaboration, and pretty UI.  \n",
    "But for some reason, they made it really hard to work with text/markdown blocks.\n",
    "\n",
    "We ended up working locally in VSCode by setting up a virtual environment,  \n",
    "and using the Live Share extension for realtime collaboration.  \n",
    "It wasn't super hard to set up, and is pretty easy to work with.  \n",
    "So in the future, we will start with this option."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    },
    {
     "sourceId": 211253469,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 211322530,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1443.000245,
   "end_time": "2024-12-22T18:22:59.262699",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-22T17:58:56.262454",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
